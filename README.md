# Language-Based-Robot-Manipulation

The system demonstrates multimodal integration for human-robot interaction. It enables voice-guided object detection and real-time pick-and-place manipulation using a robotic arm. A YOLOWorld model identifies objects from spoken commands, maps image coordinates to robot coordinates, and autonomously executes actions, allowing the user to perform continuous tasks through natural interaction.
