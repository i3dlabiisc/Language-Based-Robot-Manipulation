# Language-Based-Robot-Manipulation

The system enables voice-guided object detection and pick-and-place manipulation using a robotic arm. A YOLOWorld model identifies objects from spoken commands, maps image coordinates to robot space, and autonomously executes actions, demonstrating multimodal integration for human-robot interaction.
![image](https://github.com/user-attachments/assets/ddb386ea-3abe-420f-8b66-f8953452d187)
